{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31762028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6077ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conc</th>\n",
       "      <th>Data</th>\n",
       "      <th>NR1</th>\n",
       "      <th>NR2</th>\n",
       "      <th>NR3</th>\n",
       "      <th>NR4</th>\n",
       "      <th>NR5</th>\n",
       "      <th>NR6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2601</td>\n",
       "      <td>14/06/2023</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2600</td>\n",
       "      <td>10/06/2023</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>46</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2599</td>\n",
       "      <td>07/06/2023</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2598</td>\n",
       "      <td>03/06/2023</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>53</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2597</td>\n",
       "      <td>31/05/2023</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>56</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Conc        Data  NR1  NR2  NR3  NR4  NR5  NR6\n",
       "0  2601  14/06/2023    3    8   34   40   44   55\n",
       "1  2600  10/06/2023    4   18   37   38   46   60\n",
       "2  2599  07/06/2023   23   28   34   43   47   60\n",
       "3  2598  03/06/2023    7   14   24   53   58   60\n",
       "4  2597  31/05/2023   14   26   34   54   56   58"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Resultados-MegaSena.csv\", encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92482b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data['Data'] = pd.to_datetime(data['Data'], format='%d/%m/%Y')\n",
    "data['Day'] = data['Data'].dt.day\n",
    "data['Month'] = data['Data'].dt.month\n",
    "data = data.drop('Data', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6968e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset\n",
    "class LotteryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = data[['Conc', 'NR1', 'NR2', 'NR3', 'NR4', 'NR5', 'NR6', 'Day', 'Month']].values\n",
    "        self.y = data[['NR1', 'NR2', 'NR3', 'NR4', 'NR5', 'NR6']].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx]).float(), torch.tensor(self.y[idx]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c401b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "class LotteryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LotteryNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(9, 32)  # Input layer (9 features)\n",
    "        self.fc2 = nn.Linear(32, 64) # Hidden Layer 1\n",
    "        self.fc3 = nn.Linear(64, 128) # Hidden Layer 2\n",
    "        self.fc4 = nn.Linear(128, 64) # Hidden Layer 3\n",
    "        self.fc5 = nn.Linear(64, 6) # Output layer\n",
    "        \n",
    "        self.relu = nn.ReLU() # Activation function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.fc5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7ad476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model for training\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(torch_directml.device())\n",
    "model = LotteryNet().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c04921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data loader\n",
    "batch_size = 16\n",
    "dataset = LotteryDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17bf132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [1/163], Loss: 1730.2454\n",
      "Epoch [1/100], Step [21/163], Loss: 399.1405\n",
      "Epoch [1/100], Step [41/163], Loss: 319.4892\n",
      "Epoch [1/100], Step [61/163], Loss: 86.9622\n",
      "Epoch [1/100], Step [81/163], Loss: 64.7542\n",
      "Epoch [1/100], Step [101/163], Loss: 60.0641\n",
      "Epoch [1/100], Step [121/163], Loss: 40.4751\n",
      "Epoch [1/100], Step [141/163], Loss: 37.5550\n",
      "Epoch [1/100], Step [161/163], Loss: 34.3356\n",
      "Epoch [2/100], Step [1/163], Loss: 32.5176\n",
      "Epoch [2/100], Step [21/163], Loss: 40.1238\n",
      "Epoch [2/100], Step [41/163], Loss: 47.6702\n",
      "Epoch [2/100], Step [61/163], Loss: 36.0490\n",
      "Epoch [2/100], Step [81/163], Loss: 43.8821\n",
      "Epoch [2/100], Step [101/163], Loss: 34.1641\n",
      "Epoch [2/100], Step [121/163], Loss: 41.3649\n",
      "Epoch [2/100], Step [141/163], Loss: 33.1075\n",
      "Epoch [2/100], Step [161/163], Loss: 41.1636\n",
      "Epoch [3/100], Step [1/163], Loss: 40.9848\n",
      "Epoch [3/100], Step [21/163], Loss: 52.1539\n",
      "Epoch [3/100], Step [41/163], Loss: 36.5810\n",
      "Epoch [3/100], Step [61/163], Loss: 32.7996\n",
      "Epoch [3/100], Step [81/163], Loss: 25.8348\n",
      "Epoch [3/100], Step [101/163], Loss: 43.4145\n",
      "Epoch [3/100], Step [121/163], Loss: 54.6750\n",
      "Epoch [3/100], Step [141/163], Loss: 38.7701\n",
      "Epoch [3/100], Step [161/163], Loss: 42.1477\n",
      "Epoch [4/100], Step [1/163], Loss: 28.0870\n",
      "Epoch [4/100], Step [21/163], Loss: 36.7552\n",
      "Epoch [4/100], Step [41/163], Loss: 34.3590\n",
      "Epoch [4/100], Step [61/163], Loss: 46.8392\n",
      "Epoch [4/100], Step [81/163], Loss: 43.1746\n",
      "Epoch [4/100], Step [101/163], Loss: 34.1491\n",
      "Epoch [4/100], Step [121/163], Loss: 32.4774\n",
      "Epoch [4/100], Step [141/163], Loss: 40.7845\n",
      "Epoch [4/100], Step [161/163], Loss: 33.6686\n",
      "Epoch [5/100], Step [1/163], Loss: 33.5481\n",
      "Epoch [5/100], Step [21/163], Loss: 24.3437\n",
      "Epoch [5/100], Step [41/163], Loss: 16.7285\n",
      "Epoch [5/100], Step [61/163], Loss: 20.9518\n",
      "Epoch [5/100], Step [81/163], Loss: 26.4105\n",
      "Epoch [5/100], Step [101/163], Loss: 16.2251\n",
      "Epoch [5/100], Step [121/163], Loss: 20.7524\n",
      "Epoch [5/100], Step [141/163], Loss: 18.6029\n",
      "Epoch [5/100], Step [161/163], Loss: 18.6213\n",
      "Epoch [6/100], Step [1/163], Loss: 21.3701\n",
      "Epoch [6/100], Step [21/163], Loss: 16.4270\n",
      "Epoch [6/100], Step [41/163], Loss: 19.0215\n",
      "Epoch [6/100], Step [61/163], Loss: 14.2689\n",
      "Epoch [6/100], Step [81/163], Loss: 18.3958\n",
      "Epoch [6/100], Step [101/163], Loss: 12.0078\n",
      "Epoch [6/100], Step [121/163], Loss: 15.8246\n",
      "Epoch [6/100], Step [141/163], Loss: 11.9295\n",
      "Epoch [6/100], Step [161/163], Loss: 16.3603\n",
      "Epoch [7/100], Step [1/163], Loss: 17.7267\n",
      "Epoch [7/100], Step [21/163], Loss: 11.0457\n",
      "Epoch [7/100], Step [41/163], Loss: 10.9928\n",
      "Epoch [7/100], Step [61/163], Loss: 13.3492\n",
      "Epoch [7/100], Step [81/163], Loss: 8.7351\n",
      "Epoch [7/100], Step [101/163], Loss: 12.0082\n",
      "Epoch [7/100], Step [121/163], Loss: 10.1624\n",
      "Epoch [7/100], Step [141/163], Loss: 10.0642\n",
      "Epoch [7/100], Step [161/163], Loss: 11.1707\n",
      "Epoch [8/100], Step [1/163], Loss: 12.4257\n",
      "Epoch [8/100], Step [21/163], Loss: 14.2170\n",
      "Epoch [8/100], Step [41/163], Loss: 10.1995\n",
      "Epoch [8/100], Step [61/163], Loss: 11.0971\n",
      "Epoch [8/100], Step [81/163], Loss: 8.0338\n",
      "Epoch [8/100], Step [101/163], Loss: 11.1442\n",
      "Epoch [8/100], Step [121/163], Loss: 9.9213\n",
      "Epoch [8/100], Step [141/163], Loss: 11.6385\n",
      "Epoch [8/100], Step [161/163], Loss: 12.0545\n",
      "Epoch [9/100], Step [1/163], Loss: 10.3722\n",
      "Epoch [9/100], Step [21/163], Loss: 7.8989\n",
      "Epoch [9/100], Step [41/163], Loss: 12.1533\n",
      "Epoch [9/100], Step [61/163], Loss: 10.5534\n",
      "Epoch [9/100], Step [81/163], Loss: 6.7118\n",
      "Epoch [9/100], Step [101/163], Loss: 6.6138\n",
      "Epoch [9/100], Step [121/163], Loss: 8.0939\n",
      "Epoch [9/100], Step [141/163], Loss: 15.3786\n",
      "Epoch [9/100], Step [161/163], Loss: 8.5550\n",
      "Epoch [10/100], Step [1/163], Loss: 7.5775\n",
      "Epoch [10/100], Step [21/163], Loss: 3.9744\n",
      "Epoch [10/100], Step [41/163], Loss: 8.1364\n",
      "Epoch [10/100], Step [61/163], Loss: 7.6901\n",
      "Epoch [10/100], Step [81/163], Loss: 4.5375\n",
      "Epoch [10/100], Step [101/163], Loss: 7.3641\n",
      "Epoch [10/100], Step [121/163], Loss: 5.3760\n",
      "Epoch [10/100], Step [141/163], Loss: 6.8759\n",
      "Epoch [10/100], Step [161/163], Loss: 7.9450\n",
      "Epoch [11/100], Step [1/163], Loss: 10.6175\n",
      "Epoch [11/100], Step [21/163], Loss: 6.1080\n",
      "Epoch [11/100], Step [41/163], Loss: 5.6580\n",
      "Epoch [11/100], Step [61/163], Loss: 7.7900\n",
      "Epoch [11/100], Step [81/163], Loss: 5.7867\n",
      "Epoch [11/100], Step [101/163], Loss: 8.2360\n",
      "Epoch [11/100], Step [121/163], Loss: 8.1088\n",
      "Epoch [11/100], Step [141/163], Loss: 6.1194\n",
      "Epoch [11/100], Step [161/163], Loss: 6.8076\n",
      "Epoch [12/100], Step [1/163], Loss: 4.5085\n",
      "Epoch [12/100], Step [21/163], Loss: 6.3449\n",
      "Epoch [12/100], Step [41/163], Loss: 6.3799\n",
      "Epoch [12/100], Step [61/163], Loss: 6.1867\n",
      "Epoch [12/100], Step [81/163], Loss: 6.0474\n",
      "Epoch [12/100], Step [101/163], Loss: 5.6444\n",
      "Epoch [12/100], Step [121/163], Loss: 3.2015\n",
      "Epoch [12/100], Step [141/163], Loss: 2.5252\n",
      "Epoch [12/100], Step [161/163], Loss: 6.8270\n",
      "Epoch [13/100], Step [1/163], Loss: 5.5335\n",
      "Epoch [13/100], Step [21/163], Loss: 3.1193\n",
      "Epoch [13/100], Step [41/163], Loss: 3.5352\n",
      "Epoch [13/100], Step [61/163], Loss: 5.6786\n",
      "Epoch [13/100], Step [81/163], Loss: 2.1160\n",
      "Epoch [13/100], Step [101/163], Loss: 2.5476\n",
      "Epoch [13/100], Step [121/163], Loss: 5.7156\n",
      "Epoch [13/100], Step [141/163], Loss: 4.4849\n",
      "Epoch [13/100], Step [161/163], Loss: 4.4741\n",
      "Epoch [14/100], Step [1/163], Loss: 4.8868\n",
      "Epoch [14/100], Step [21/163], Loss: 6.2877\n",
      "Epoch [14/100], Step [41/163], Loss: 5.8394\n",
      "Epoch [14/100], Step [61/163], Loss: 3.0317\n",
      "Epoch [14/100], Step [81/163], Loss: 5.2260\n",
      "Epoch [14/100], Step [101/163], Loss: 3.3632\n",
      "Epoch [14/100], Step [121/163], Loss: 4.0929\n",
      "Epoch [14/100], Step [141/163], Loss: 3.9285\n",
      "Epoch [14/100], Step [161/163], Loss: 2.2669\n",
      "Epoch [15/100], Step [1/163], Loss: 2.6205\n",
      "Epoch [15/100], Step [21/163], Loss: 1.9242\n",
      "Epoch [15/100], Step [41/163], Loss: 4.5419\n",
      "Epoch [15/100], Step [61/163], Loss: 2.5065\n",
      "Epoch [15/100], Step [81/163], Loss: 3.0896\n",
      "Epoch [15/100], Step [101/163], Loss: 4.3937\n",
      "Epoch [15/100], Step [121/163], Loss: 2.5942\n",
      "Epoch [15/100], Step [141/163], Loss: 3.0953\n",
      "Epoch [15/100], Step [161/163], Loss: 2.4964\n",
      "Epoch [16/100], Step [1/163], Loss: 1.8707\n",
      "Epoch [16/100], Step [21/163], Loss: 4.2577\n",
      "Epoch [16/100], Step [41/163], Loss: 7.6289\n",
      "Epoch [16/100], Step [61/163], Loss: 3.7372\n",
      "Epoch [16/100], Step [81/163], Loss: 2.5975\n",
      "Epoch [16/100], Step [101/163], Loss: 2.7977\n",
      "Epoch [16/100], Step [121/163], Loss: 2.7734\n",
      "Epoch [16/100], Step [141/163], Loss: 2.4591\n",
      "Epoch [16/100], Step [161/163], Loss: 2.1268\n",
      "Epoch [17/100], Step [1/163], Loss: 4.2894\n",
      "Epoch [17/100], Step [21/163], Loss: 2.3755\n",
      "Epoch [17/100], Step [41/163], Loss: 5.0718\n",
      "Epoch [17/100], Step [61/163], Loss: 3.0438\n",
      "Epoch [17/100], Step [81/163], Loss: 5.1887\n",
      "Epoch [17/100], Step [101/163], Loss: 2.9784\n",
      "Epoch [17/100], Step [121/163], Loss: 2.1027\n",
      "Epoch [17/100], Step [141/163], Loss: 2.4805\n",
      "Epoch [17/100], Step [161/163], Loss: 3.2790\n",
      "Epoch [18/100], Step [1/163], Loss: 0.5162\n",
      "Epoch [18/100], Step [21/163], Loss: 4.5968\n",
      "Epoch [18/100], Step [41/163], Loss: 3.7113\n",
      "Epoch [18/100], Step [61/163], Loss: 4.6640\n",
      "Epoch [18/100], Step [81/163], Loss: 3.3114\n",
      "Epoch [18/100], Step [101/163], Loss: 2.7984\n",
      "Epoch [18/100], Step [121/163], Loss: 2.7775\n",
      "Epoch [18/100], Step [141/163], Loss: 5.3785\n",
      "Epoch [18/100], Step [161/163], Loss: 2.1848\n",
      "Epoch [19/100], Step [1/163], Loss: 2.0907\n",
      "Epoch [19/100], Step [21/163], Loss: 2.5834\n",
      "Epoch [19/100], Step [41/163], Loss: 3.2880\n",
      "Epoch [19/100], Step [61/163], Loss: 2.4091\n",
      "Epoch [19/100], Step [81/163], Loss: 3.8089\n",
      "Epoch [19/100], Step [101/163], Loss: 10.8085\n",
      "Epoch [19/100], Step [121/163], Loss: 4.8835\n",
      "Epoch [19/100], Step [141/163], Loss: 3.9137\n",
      "Epoch [19/100], Step [161/163], Loss: 2.9069\n",
      "Epoch [20/100], Step [1/163], Loss: 3.2392\n",
      "Epoch [20/100], Step [21/163], Loss: 1.8872\n",
      "Epoch [20/100], Step [41/163], Loss: 1.6925\n",
      "Epoch [20/100], Step [61/163], Loss: 3.1837\n",
      "Epoch [20/100], Step [81/163], Loss: 3.7050\n",
      "Epoch [20/100], Step [101/163], Loss: 2.8898\n",
      "Epoch [20/100], Step [121/163], Loss: 3.1624\n",
      "Epoch [20/100], Step [141/163], Loss: 3.2793\n",
      "Epoch [20/100], Step [161/163], Loss: 3.1470\n",
      "Epoch [21/100], Step [1/163], Loss: 1.8558\n",
      "Epoch [21/100], Step [21/163], Loss: 4.1031\n",
      "Epoch [21/100], Step [41/163], Loss: 2.5930\n",
      "Epoch [21/100], Step [61/163], Loss: 1.6576\n",
      "Epoch [21/100], Step [81/163], Loss: 1.9226\n",
      "Epoch [21/100], Step [101/163], Loss: 1.6290\n",
      "Epoch [21/100], Step [121/163], Loss: 1.9952\n",
      "Epoch [21/100], Step [141/163], Loss: 1.7228\n",
      "Epoch [21/100], Step [161/163], Loss: 3.5542\n",
      "Epoch [22/100], Step [1/163], Loss: 2.0463\n",
      "Epoch [22/100], Step [21/163], Loss: 1.7023\n",
      "Epoch [22/100], Step [41/163], Loss: 1.6244\n",
      "Epoch [22/100], Step [61/163], Loss: 3.3547\n",
      "Epoch [22/100], Step [81/163], Loss: 2.6769\n",
      "Epoch [22/100], Step [101/163], Loss: 1.7444\n",
      "Epoch [22/100], Step [121/163], Loss: 1.9896\n",
      "Epoch [22/100], Step [141/163], Loss: 1.9989\n",
      "Epoch [22/100], Step [161/163], Loss: 1.6826\n",
      "Epoch [23/100], Step [1/163], Loss: 1.0312\n",
      "Epoch [23/100], Step [21/163], Loss: 1.8536\n",
      "Epoch [23/100], Step [41/163], Loss: 1.2888\n",
      "Epoch [23/100], Step [61/163], Loss: 0.8291\n",
      "Epoch [23/100], Step [81/163], Loss: 0.5744\n",
      "Epoch [23/100], Step [101/163], Loss: 0.8661\n",
      "Epoch [23/100], Step [121/163], Loss: 0.8541\n",
      "Epoch [23/100], Step [141/163], Loss: 1.2042\n",
      "Epoch [23/100], Step [161/163], Loss: 3.5121\n",
      "Epoch [24/100], Step [1/163], Loss: 1.4389\n",
      "Epoch [24/100], Step [21/163], Loss: 1.1873\n",
      "Epoch [24/100], Step [41/163], Loss: 0.7148\n",
      "Epoch [24/100], Step [61/163], Loss: 0.3529\n",
      "Epoch [24/100], Step [81/163], Loss: 0.4509\n",
      "Epoch [24/100], Step [101/163], Loss: 0.5163\n",
      "Epoch [24/100], Step [121/163], Loss: 0.3087\n",
      "Epoch [24/100], Step [141/163], Loss: 0.4440\n",
      "Epoch [24/100], Step [161/163], Loss: 0.2772\n",
      "Epoch [25/100], Step [1/163], Loss: 0.3250\n",
      "Epoch [25/100], Step [21/163], Loss: 0.2815\n",
      "Epoch [25/100], Step [41/163], Loss: 2.8305\n",
      "Epoch [25/100], Step [61/163], Loss: 3.9933\n",
      "Epoch [25/100], Step [81/163], Loss: 1.3467\n",
      "Epoch [25/100], Step [101/163], Loss: 0.7151\n",
      "Epoch [25/100], Step [121/163], Loss: 0.1833\n",
      "Epoch [25/100], Step [141/163], Loss: 0.2235\n",
      "Epoch [25/100], Step [161/163], Loss: 0.1455\n",
      "Epoch [26/100], Step [1/163], Loss: 0.2038\n",
      "Epoch [26/100], Step [21/163], Loss: 0.1919\n",
      "Epoch [26/100], Step [41/163], Loss: 1.5082\n",
      "Epoch [26/100], Step [61/163], Loss: 0.2975\n",
      "Epoch [26/100], Step [81/163], Loss: 0.3177\n",
      "Epoch [26/100], Step [101/163], Loss: 0.1326\n",
      "Epoch [26/100], Step [121/163], Loss: 0.1604\n",
      "Epoch [26/100], Step [141/163], Loss: 0.1183\n",
      "Epoch [26/100], Step [161/163], Loss: 0.2788\n",
      "Epoch [27/100], Step [1/163], Loss: 0.1649\n",
      "Epoch [27/100], Step [21/163], Loss: 0.3064\n",
      "Epoch [27/100], Step [41/163], Loss: 0.1463\n",
      "Epoch [27/100], Step [61/163], Loss: 0.1571\n",
      "Epoch [27/100], Step [81/163], Loss: 1.0616\n",
      "Epoch [27/100], Step [101/163], Loss: 1.0025\n",
      "Epoch [27/100], Step [121/163], Loss: 0.3671\n",
      "Epoch [27/100], Step [141/163], Loss: 0.6858\n",
      "Epoch [27/100], Step [161/163], Loss: 0.2177\n",
      "Epoch [28/100], Step [1/163], Loss: 0.2242\n",
      "Epoch [28/100], Step [21/163], Loss: 0.2263\n",
      "Epoch [28/100], Step [41/163], Loss: 0.0632\n",
      "Epoch [28/100], Step [61/163], Loss: 0.1200\n",
      "Epoch [28/100], Step [81/163], Loss: 0.1485\n",
      "Epoch [28/100], Step [101/163], Loss: 0.6774\n",
      "Epoch [28/100], Step [121/163], Loss: 0.5086\n",
      "Epoch [28/100], Step [141/163], Loss: 0.0711\n",
      "Epoch [28/100], Step [161/163], Loss: 0.1776\n",
      "Epoch [29/100], Step [1/163], Loss: 0.0679\n",
      "Epoch [29/100], Step [21/163], Loss: 0.0577\n",
      "Epoch [29/100], Step [41/163], Loss: 0.1425\n",
      "Epoch [29/100], Step [61/163], Loss: 0.0821\n",
      "Epoch [29/100], Step [81/163], Loss: 0.0480\n",
      "Epoch [29/100], Step [101/163], Loss: 0.4192\n",
      "Epoch [29/100], Step [121/163], Loss: 0.5819\n",
      "Epoch [29/100], Step [141/163], Loss: 1.2412\n",
      "Epoch [29/100], Step [161/163], Loss: 0.2693\n",
      "Epoch [30/100], Step [1/163], Loss: 1.0736\n",
      "Epoch [30/100], Step [21/163], Loss: 0.2328\n",
      "Epoch [30/100], Step [41/163], Loss: 0.1418\n",
      "Epoch [30/100], Step [61/163], Loss: 0.0419\n",
      "Epoch [30/100], Step [81/163], Loss: 0.4090\n",
      "Epoch [30/100], Step [101/163], Loss: 0.2491\n",
      "Epoch [30/100], Step [121/163], Loss: 0.0382\n",
      "Epoch [30/100], Step [141/163], Loss: 0.1426\n",
      "Epoch [30/100], Step [161/163], Loss: 0.1558\n",
      "Epoch [31/100], Step [1/163], Loss: 0.0884\n",
      "Epoch [31/100], Step [21/163], Loss: 0.0347\n",
      "Epoch [31/100], Step [41/163], Loss: 0.0252\n",
      "Epoch [31/100], Step [61/163], Loss: 0.0381\n",
      "Epoch [31/100], Step [81/163], Loss: 0.0786\n",
      "Epoch [31/100], Step [101/163], Loss: 0.1643\n",
      "Epoch [31/100], Step [121/163], Loss: 0.2181\n",
      "Epoch [31/100], Step [141/163], Loss: 0.0597\n",
      "Epoch [31/100], Step [161/163], Loss: 0.1489\n",
      "Epoch [32/100], Step [1/163], Loss: 0.5888\n",
      "Epoch [32/100], Step [21/163], Loss: 0.1563\n",
      "Epoch [32/100], Step [41/163], Loss: 0.2661\n",
      "Epoch [32/100], Step [61/163], Loss: 0.1658\n",
      "Epoch [32/100], Step [81/163], Loss: 8.9643\n",
      "Epoch [32/100], Step [101/163], Loss: 3.2133\n",
      "Epoch [32/100], Step [121/163], Loss: 1.2115\n",
      "Epoch [32/100], Step [141/163], Loss: 0.9111\n",
      "Epoch [32/100], Step [161/163], Loss: 0.3010\n",
      "Epoch [33/100], Step [1/163], Loss: 0.4800\n",
      "Epoch [33/100], Step [21/163], Loss: 0.2496\n",
      "Epoch [33/100], Step [41/163], Loss: 0.1312\n",
      "Epoch [33/100], Step [61/163], Loss: 0.1618\n",
      "Epoch [33/100], Step [81/163], Loss: 0.0871\n",
      "Epoch [33/100], Step [101/163], Loss: 0.0700\n",
      "Epoch [33/100], Step [121/163], Loss: 0.1287\n",
      "Epoch [33/100], Step [141/163], Loss: 0.1816\n",
      "Epoch [33/100], Step [161/163], Loss: 0.1232\n",
      "Epoch [34/100], Step [1/163], Loss: 0.0886\n",
      "Epoch [34/100], Step [21/163], Loss: 0.1447\n",
      "Epoch [34/100], Step [41/163], Loss: 0.0411\n",
      "Epoch [34/100], Step [61/163], Loss: 0.2702\n",
      "Epoch [34/100], Step [81/163], Loss: 0.1624\n",
      "Epoch [34/100], Step [101/163], Loss: 0.0509\n",
      "Epoch [34/100], Step [121/163], Loss: 0.0881\n",
      "Epoch [34/100], Step [141/163], Loss: 0.0800\n",
      "Epoch [34/100], Step [161/163], Loss: 0.1244\n",
      "Epoch [35/100], Step [1/163], Loss: 0.0853\n",
      "Epoch [35/100], Step [21/163], Loss: 0.3043\n",
      "Epoch [35/100], Step [41/163], Loss: 0.0725\n",
      "Epoch [35/100], Step [61/163], Loss: 0.0485\n",
      "Epoch [35/100], Step [81/163], Loss: 0.3180\n",
      "Epoch [35/100], Step [101/163], Loss: 0.1119\n",
      "Epoch [35/100], Step [121/163], Loss: 0.0340\n",
      "Epoch [35/100], Step [141/163], Loss: 0.0453\n",
      "Epoch [35/100], Step [161/163], Loss: 0.0398\n",
      "Epoch [36/100], Step [1/163], Loss: 0.0953\n",
      "Epoch [36/100], Step [21/163], Loss: 0.6573\n",
      "Epoch [36/100], Step [41/163], Loss: 1.4012\n",
      "Epoch [36/100], Step [61/163], Loss: 0.1738\n",
      "Epoch [36/100], Step [81/163], Loss: 0.1143\n",
      "Epoch [36/100], Step [101/163], Loss: 0.1827\n",
      "Epoch [36/100], Step [121/163], Loss: 0.2102\n",
      "Epoch [36/100], Step [141/163], Loss: 0.0915\n",
      "Epoch [36/100], Step [161/163], Loss: 0.2573\n",
      "Epoch [37/100], Step [1/163], Loss: 0.4676\n",
      "Epoch [37/100], Step [21/163], Loss: 0.1381\n",
      "Epoch [37/100], Step [41/163], Loss: 0.1429\n",
      "Epoch [37/100], Step [61/163], Loss: 0.1081\n",
      "Epoch [37/100], Step [81/163], Loss: 0.1981\n",
      "Epoch [37/100], Step [101/163], Loss: 0.0919\n",
      "Epoch [37/100], Step [121/163], Loss: 0.1037\n",
      "Epoch [37/100], Step [141/163], Loss: 0.0677\n",
      "Epoch [37/100], Step [161/163], Loss: 0.0846\n",
      "Epoch [38/100], Step [1/163], Loss: 0.1450\n",
      "Epoch [38/100], Step [21/163], Loss: 0.1352\n",
      "Epoch [38/100], Step [41/163], Loss: 0.3930\n",
      "Epoch [38/100], Step [61/163], Loss: 0.1993\n",
      "Epoch [38/100], Step [81/163], Loss: 0.1357\n",
      "Epoch [38/100], Step [101/163], Loss: 0.0295\n",
      "Epoch [38/100], Step [121/163], Loss: 0.1342\n",
      "Epoch [38/100], Step [141/163], Loss: 0.4218\n",
      "Epoch [38/100], Step [161/163], Loss: 0.4136\n",
      "Epoch [39/100], Step [1/163], Loss: 0.6260\n",
      "Epoch [39/100], Step [21/163], Loss: 0.4833\n",
      "Epoch [39/100], Step [41/163], Loss: 0.3925\n",
      "Epoch [39/100], Step [61/163], Loss: 0.2053\n",
      "Epoch [39/100], Step [81/163], Loss: 0.5233\n",
      "Epoch [39/100], Step [101/163], Loss: 0.2919\n",
      "Epoch [39/100], Step [121/163], Loss: 0.0440\n",
      "Epoch [39/100], Step [141/163], Loss: 0.0537\n",
      "Epoch [39/100], Step [161/163], Loss: 0.0702\n",
      "Epoch [40/100], Step [1/163], Loss: 0.0579\n",
      "Epoch [40/100], Step [21/163], Loss: 0.0751\n",
      "Epoch [40/100], Step [41/163], Loss: 0.0729\n",
      "Epoch [40/100], Step [61/163], Loss: 0.0780\n",
      "Epoch [40/100], Step [81/163], Loss: 0.0513\n",
      "Epoch [40/100], Step [101/163], Loss: 0.1374\n",
      "Epoch [40/100], Step [121/163], Loss: 0.0779\n",
      "Epoch [40/100], Step [141/163], Loss: 0.1117\n",
      "Epoch [40/100], Step [161/163], Loss: 0.1308\n",
      "Epoch [41/100], Step [1/163], Loss: 0.2104\n",
      "Epoch [41/100], Step [21/163], Loss: 0.0921\n",
      "Epoch [41/100], Step [41/163], Loss: 0.0681\n",
      "Epoch [41/100], Step [61/163], Loss: 0.0842\n",
      "Epoch [41/100], Step [81/163], Loss: 0.0966\n",
      "Epoch [41/100], Step [101/163], Loss: 0.3899\n",
      "Epoch [41/100], Step [121/163], Loss: 1.0700\n",
      "Epoch [41/100], Step [141/163], Loss: 0.4578\n",
      "Epoch [41/100], Step [161/163], Loss: 0.3345\n",
      "Epoch [42/100], Step [1/163], Loss: 0.1965\n",
      "Epoch [42/100], Step [21/163], Loss: 0.2147\n",
      "Epoch [42/100], Step [41/163], Loss: 0.0988\n",
      "Epoch [42/100], Step [61/163], Loss: 0.1421\n",
      "Epoch [42/100], Step [81/163], Loss: 0.2823\n",
      "Epoch [42/100], Step [101/163], Loss: 0.1453\n",
      "Epoch [42/100], Step [121/163], Loss: 0.0576\n",
      "Epoch [42/100], Step [141/163], Loss: 0.1499\n",
      "Epoch [42/100], Step [161/163], Loss: 0.1313\n",
      "Epoch [43/100], Step [1/163], Loss: 0.5421\n",
      "Epoch [43/100], Step [21/163], Loss: 1.1171\n",
      "Epoch [43/100], Step [41/163], Loss: 2.5749\n",
      "Epoch [43/100], Step [61/163], Loss: 0.2674\n",
      "Epoch [43/100], Step [81/163], Loss: 0.1054\n",
      "Epoch [43/100], Step [101/163], Loss: 0.2238\n",
      "Epoch [43/100], Step [121/163], Loss: 1.2377\n",
      "Epoch [43/100], Step [141/163], Loss: 0.1699\n",
      "Epoch [43/100], Step [161/163], Loss: 0.0503\n",
      "Epoch [44/100], Step [1/163], Loss: 0.0579\n",
      "Epoch [44/100], Step [21/163], Loss: 0.1914\n",
      "Epoch [44/100], Step [41/163], Loss: 0.1097\n",
      "Epoch [44/100], Step [61/163], Loss: 0.0930\n",
      "Epoch [44/100], Step [81/163], Loss: 0.0237\n",
      "Epoch [44/100], Step [101/163], Loss: 0.0294\n",
      "Epoch [44/100], Step [121/163], Loss: 0.0586\n",
      "Epoch [44/100], Step [141/163], Loss: 0.0353\n",
      "Epoch [44/100], Step [161/163], Loss: 0.1324\n",
      "Epoch [45/100], Step [1/163], Loss: 0.0841\n",
      "Epoch [45/100], Step [21/163], Loss: 0.0484\n",
      "Epoch [45/100], Step [41/163], Loss: 0.0444\n",
      "Epoch [45/100], Step [61/163], Loss: 0.0253\n",
      "Epoch [45/100], Step [81/163], Loss: 0.0527\n",
      "Epoch [45/100], Step [101/163], Loss: 0.0222\n",
      "Epoch [45/100], Step [121/163], Loss: 0.0513\n",
      "Epoch [45/100], Step [141/163], Loss: 0.0359\n",
      "Epoch [45/100], Step [161/163], Loss: 0.2020\n",
      "Epoch [46/100], Step [1/163], Loss: 0.1183\n",
      "Epoch [46/100], Step [21/163], Loss: 0.0813\n",
      "Epoch [46/100], Step [41/163], Loss: 0.1526\n",
      "Epoch [46/100], Step [61/163], Loss: 0.0996\n",
      "Epoch [46/100], Step [81/163], Loss: 0.1271\n",
      "Epoch [46/100], Step [101/163], Loss: 0.0259\n",
      "Epoch [46/100], Step [121/163], Loss: 0.2057\n",
      "Epoch [46/100], Step [141/163], Loss: 0.2956\n",
      "Epoch [46/100], Step [161/163], Loss: 0.1681\n",
      "Epoch [47/100], Step [1/163], Loss: 0.3245\n",
      "Epoch [47/100], Step [21/163], Loss: 2.6843\n",
      "Epoch [47/100], Step [41/163], Loss: 0.2034\n",
      "Epoch [47/100], Step [61/163], Loss: 0.1877\n",
      "Epoch [47/100], Step [81/163], Loss: 1.0741\n",
      "Epoch [47/100], Step [101/163], Loss: 0.4903\n",
      "Epoch [47/100], Step [121/163], Loss: 0.2668\n",
      "Epoch [47/100], Step [141/163], Loss: 0.4451\n",
      "Epoch [47/100], Step [161/163], Loss: 0.1602\n",
      "Epoch [48/100], Step [1/163], Loss: 0.3146\n",
      "Epoch [48/100], Step [21/163], Loss: 0.1996\n",
      "Epoch [48/100], Step [41/163], Loss: 1.6692\n",
      "Epoch [48/100], Step [61/163], Loss: 0.0687\n",
      "Epoch [48/100], Step [81/163], Loss: 0.0597\n",
      "Epoch [48/100], Step [101/163], Loss: 0.1452\n",
      "Epoch [48/100], Step [121/163], Loss: 0.1377\n",
      "Epoch [48/100], Step [141/163], Loss: 0.6642\n",
      "Epoch [48/100], Step [161/163], Loss: 0.3282\n",
      "Epoch [49/100], Step [1/163], Loss: 1.1136\n",
      "Epoch [49/100], Step [21/163], Loss: 0.7151\n",
      "Epoch [49/100], Step [41/163], Loss: 0.2111\n",
      "Epoch [49/100], Step [61/163], Loss: 0.3632\n",
      "Epoch [49/100], Step [81/163], Loss: 0.3442\n",
      "Epoch [49/100], Step [101/163], Loss: 0.0692\n",
      "Epoch [49/100], Step [121/163], Loss: 0.1110\n",
      "Epoch [49/100], Step [141/163], Loss: 0.0437\n",
      "Epoch [49/100], Step [161/163], Loss: 0.0163\n",
      "Epoch [50/100], Step [1/163], Loss: 0.0233\n",
      "Epoch [50/100], Step [21/163], Loss: 0.0281\n",
      "Epoch [50/100], Step [41/163], Loss: 0.0237\n",
      "Epoch [50/100], Step [61/163], Loss: 0.1861\n",
      "Epoch [50/100], Step [81/163], Loss: 0.1269\n",
      "Epoch [50/100], Step [101/163], Loss: 0.1107\n",
      "Epoch [50/100], Step [121/163], Loss: 0.0956\n",
      "Epoch [50/100], Step [141/163], Loss: 0.0389\n",
      "Epoch [50/100], Step [161/163], Loss: 0.2167\n",
      "Epoch [51/100], Step [1/163], Loss: 0.1160\n",
      "Epoch [51/100], Step [21/163], Loss: 0.0303\n",
      "Epoch [51/100], Step [41/163], Loss: 0.3655\n",
      "Epoch [51/100], Step [61/163], Loss: 0.4858\n",
      "Epoch [51/100], Step [81/163], Loss: 0.4058\n",
      "Epoch [51/100], Step [101/163], Loss: 0.3455\n",
      "Epoch [51/100], Step [121/163], Loss: 0.0969\n",
      "Epoch [51/100], Step [141/163], Loss: 0.1370\n",
      "Epoch [51/100], Step [161/163], Loss: 0.1537\n",
      "Epoch [52/100], Step [1/163], Loss: 0.1081\n",
      "Epoch [52/100], Step [21/163], Loss: 0.1249\n",
      "Epoch [52/100], Step [41/163], Loss: 0.3051\n",
      "Epoch [52/100], Step [61/163], Loss: 0.0590\n",
      "Epoch [52/100], Step [81/163], Loss: 0.1391\n",
      "Epoch [52/100], Step [101/163], Loss: 0.1509\n",
      "Epoch [52/100], Step [121/163], Loss: 0.1444\n",
      "Epoch [52/100], Step [141/163], Loss: 0.0658\n",
      "Epoch [52/100], Step [161/163], Loss: 0.2084\n",
      "Epoch [53/100], Step [1/163], Loss: 0.1166\n",
      "Epoch [53/100], Step [21/163], Loss: 0.1041\n",
      "Epoch [53/100], Step [41/163], Loss: 0.0324\n",
      "Epoch [53/100], Step [61/163], Loss: 0.0924\n",
      "Epoch [53/100], Step [81/163], Loss: 0.0170\n",
      "Epoch [53/100], Step [101/163], Loss: 0.1043\n",
      "Epoch [53/100], Step [121/163], Loss: 0.1031\n",
      "Epoch [53/100], Step [141/163], Loss: 0.0516\n",
      "Epoch [53/100], Step [161/163], Loss: 0.0630\n",
      "Epoch [54/100], Step [1/163], Loss: 0.2447\n",
      "Epoch [54/100], Step [21/163], Loss: 0.2544\n",
      "Epoch [54/100], Step [41/163], Loss: 0.1157\n",
      "Epoch [54/100], Step [61/163], Loss: 0.4870\n",
      "Epoch [54/100], Step [81/163], Loss: 0.0901\n",
      "Epoch [54/100], Step [101/163], Loss: 0.1020\n",
      "Epoch [54/100], Step [121/163], Loss: 0.0214\n",
      "Epoch [54/100], Step [141/163], Loss: 0.0214\n",
      "Epoch [54/100], Step [161/163], Loss: 0.0381\n",
      "Epoch [55/100], Step [1/163], Loss: 0.0197\n",
      "Epoch [55/100], Step [21/163], Loss: 0.0952\n",
      "Epoch [55/100], Step [41/163], Loss: 0.2371\n",
      "Epoch [55/100], Step [61/163], Loss: 0.5227\n",
      "Epoch [55/100], Step [81/163], Loss: 0.2413\n",
      "Epoch [55/100], Step [101/163], Loss: 0.5052\n",
      "Epoch [55/100], Step [121/163], Loss: 0.6880\n",
      "Epoch [55/100], Step [141/163], Loss: 0.3860\n",
      "Epoch [55/100], Step [161/163], Loss: 0.1525\n",
      "Epoch [56/100], Step [1/163], Loss: 0.1723\n",
      "Epoch [56/100], Step [21/163], Loss: 0.1732\n",
      "Epoch [56/100], Step [41/163], Loss: 0.0432\n",
      "Epoch [56/100], Step [61/163], Loss: 0.0246\n",
      "Epoch [56/100], Step [81/163], Loss: 0.1358\n",
      "Epoch [56/100], Step [101/163], Loss: 0.0638\n",
      "Epoch [56/100], Step [121/163], Loss: 0.0338\n",
      "Epoch [56/100], Step [141/163], Loss: 0.1886\n",
      "Epoch [56/100], Step [161/163], Loss: 0.0389\n",
      "Epoch [57/100], Step [1/163], Loss: 0.0367\n",
      "Epoch [57/100], Step [21/163], Loss: 0.0174\n",
      "Epoch [57/100], Step [41/163], Loss: 0.0522\n",
      "Epoch [57/100], Step [61/163], Loss: 0.0325\n",
      "Epoch [57/100], Step [81/163], Loss: 0.0342\n",
      "Epoch [57/100], Step [101/163], Loss: 0.0913\n",
      "Epoch [57/100], Step [121/163], Loss: 0.0328\n",
      "Epoch [57/100], Step [141/163], Loss: 0.0573\n",
      "Epoch [57/100], Step [161/163], Loss: 0.0817\n",
      "Epoch [58/100], Step [1/163], Loss: 0.0299\n",
      "Epoch [58/100], Step [21/163], Loss: 0.0456\n",
      "Epoch [58/100], Step [41/163], Loss: 0.0723\n",
      "Epoch [58/100], Step [61/163], Loss: 0.0540\n",
      "Epoch [58/100], Step [81/163], Loss: 0.0517\n",
      "Epoch [58/100], Step [101/163], Loss: 0.0256\n",
      "Epoch [58/100], Step [121/163], Loss: 0.0327\n",
      "Epoch [58/100], Step [141/163], Loss: 0.2856\n",
      "Epoch [58/100], Step [161/163], Loss: 0.0414\n",
      "Epoch [59/100], Step [1/163], Loss: 0.1555\n",
      "Epoch [59/100], Step [21/163], Loss: 0.0811\n",
      "Epoch [59/100], Step [41/163], Loss: 0.0295\n",
      "Epoch [59/100], Step [61/163], Loss: 0.1647\n",
      "Epoch [59/100], Step [81/163], Loss: 0.1037\n",
      "Epoch [59/100], Step [101/163], Loss: 0.3692\n",
      "Epoch [59/100], Step [121/163], Loss: 0.2574\n",
      "Epoch [59/100], Step [141/163], Loss: 0.3451\n",
      "Epoch [59/100], Step [161/163], Loss: 0.1423\n",
      "Epoch [60/100], Step [1/163], Loss: 0.1124\n",
      "Epoch [60/100], Step [21/163], Loss: 0.2175\n",
      "Epoch [60/100], Step [41/163], Loss: 0.9421\n",
      "Epoch [60/100], Step [61/163], Loss: 0.3930\n",
      "Epoch [60/100], Step [81/163], Loss: 0.2467\n",
      "Epoch [60/100], Step [101/163], Loss: 0.1795\n",
      "Epoch [60/100], Step [121/163], Loss: 0.2126\n",
      "Epoch [60/100], Step [141/163], Loss: 0.0313\n",
      "Epoch [60/100], Step [161/163], Loss: 0.0916\n",
      "Epoch [61/100], Step [1/163], Loss: 0.1040\n",
      "Epoch [61/100], Step [21/163], Loss: 0.0155\n",
      "Epoch [61/100], Step [41/163], Loss: 0.0796\n",
      "Epoch [61/100], Step [61/163], Loss: 1.0608\n",
      "Epoch [61/100], Step [81/163], Loss: 0.1471\n",
      "Epoch [61/100], Step [101/163], Loss: 0.2099\n",
      "Epoch [61/100], Step [121/163], Loss: 0.2445\n",
      "Epoch [61/100], Step [141/163], Loss: 0.5950\n",
      "Epoch [61/100], Step [161/163], Loss: 0.2478\n",
      "Epoch [62/100], Step [1/163], Loss: 0.1359\n",
      "Epoch [62/100], Step [21/163], Loss: 0.0594\n",
      "Epoch [62/100], Step [41/163], Loss: 0.2050\n",
      "Epoch [62/100], Step [61/163], Loss: 0.0778\n",
      "Epoch [62/100], Step [81/163], Loss: 0.1106\n",
      "Epoch [62/100], Step [101/163], Loss: 0.0448\n",
      "Epoch [62/100], Step [121/163], Loss: 0.0307\n",
      "Epoch [62/100], Step [141/163], Loss: 0.0661\n",
      "Epoch [62/100], Step [161/163], Loss: 0.0530\n",
      "Epoch [63/100], Step [1/163], Loss: 0.0260\n",
      "Epoch [63/100], Step [21/163], Loss: 1.0496\n",
      "Epoch [63/100], Step [41/163], Loss: 0.2011\n",
      "Epoch [63/100], Step [61/163], Loss: 0.1947\n",
      "Epoch [63/100], Step [81/163], Loss: 0.8375\n",
      "Epoch [63/100], Step [101/163], Loss: 0.0997\n",
      "Epoch [63/100], Step [121/163], Loss: 0.1520\n",
      "Epoch [63/100], Step [141/163], Loss: 0.4035\n",
      "Epoch [63/100], Step [161/163], Loss: 0.6354\n",
      "Epoch [64/100], Step [1/163], Loss: 0.7342\n",
      "Epoch [64/100], Step [21/163], Loss: 0.0600\n",
      "Epoch [64/100], Step [41/163], Loss: 0.0497\n",
      "Epoch [64/100], Step [61/163], Loss: 0.1791\n",
      "Epoch [64/100], Step [81/163], Loss: 0.0273\n",
      "Epoch [64/100], Step [101/163], Loss: 0.0456\n",
      "Epoch [64/100], Step [121/163], Loss: 0.0167\n",
      "Epoch [64/100], Step [141/163], Loss: 0.1087\n",
      "Epoch [64/100], Step [161/163], Loss: 0.0541\n",
      "Epoch [65/100], Step [1/163], Loss: 0.0364\n",
      "Epoch [65/100], Step [21/163], Loss: 0.0684\n",
      "Epoch [65/100], Step [41/163], Loss: 0.1328\n",
      "Epoch [65/100], Step [61/163], Loss: 0.0999\n",
      "Epoch [65/100], Step [81/163], Loss: 0.1600\n",
      "Epoch [65/100], Step [101/163], Loss: 0.0098\n",
      "Epoch [65/100], Step [121/163], Loss: 0.0284\n",
      "Epoch [65/100], Step [141/163], Loss: 0.0397\n",
      "Epoch [65/100], Step [161/163], Loss: 0.0398\n",
      "Epoch [66/100], Step [1/163], Loss: 0.0228\n",
      "Epoch [66/100], Step [21/163], Loss: 0.0262\n",
      "Epoch [66/100], Step [41/163], Loss: 0.4676\n",
      "Epoch [66/100], Step [61/163], Loss: 0.0431\n",
      "Epoch [66/100], Step [81/163], Loss: 0.1422\n",
      "Epoch [66/100], Step [101/163], Loss: 0.0320\n",
      "Epoch [66/100], Step [121/163], Loss: 0.2308\n",
      "Epoch [66/100], Step [141/163], Loss: 0.1656\n",
      "Epoch [66/100], Step [161/163], Loss: 0.8657\n",
      "Epoch [67/100], Step [1/163], Loss: 1.0777\n",
      "Epoch [67/100], Step [21/163], Loss: 0.5519\n",
      "Epoch [67/100], Step [41/163], Loss: 2.3589\n",
      "Epoch [67/100], Step [61/163], Loss: 1.8545\n",
      "Epoch [67/100], Step [81/163], Loss: 0.9771\n",
      "Epoch [67/100], Step [101/163], Loss: 0.2587\n",
      "Epoch [67/100], Step [121/163], Loss: 0.3335\n",
      "Epoch [67/100], Step [141/163], Loss: 0.1742\n",
      "Epoch [67/100], Step [161/163], Loss: 0.0739\n",
      "Epoch [68/100], Step [1/163], Loss: 0.0800\n",
      "Epoch [68/100], Step [21/163], Loss: 0.0263\n",
      "Epoch [68/100], Step [41/163], Loss: 0.0835\n",
      "Epoch [68/100], Step [61/163], Loss: 0.0924\n",
      "Epoch [68/100], Step [81/163], Loss: 0.0358\n",
      "Epoch [68/100], Step [101/163], Loss: 0.0233\n",
      "Epoch [68/100], Step [121/163], Loss: 0.0108\n",
      "Epoch [68/100], Step [141/163], Loss: 0.0111\n",
      "Epoch [68/100], Step [161/163], Loss: 0.0913\n",
      "Epoch [69/100], Step [1/163], Loss: 0.0625\n",
      "Epoch [69/100], Step [21/163], Loss: 0.0446\n",
      "Epoch [69/100], Step [41/163], Loss: 0.0139\n",
      "Epoch [69/100], Step [61/163], Loss: 0.0211\n",
      "Epoch [69/100], Step [81/163], Loss: 0.1096\n",
      "Epoch [69/100], Step [101/163], Loss: 0.0330\n",
      "Epoch [69/100], Step [121/163], Loss: 0.0314\n",
      "Epoch [69/100], Step [141/163], Loss: 0.0219\n",
      "Epoch [69/100], Step [161/163], Loss: 0.0257\n",
      "Epoch [70/100], Step [1/163], Loss: 0.0827\n",
      "Epoch [70/100], Step [21/163], Loss: 0.0552\n",
      "Epoch [70/100], Step [41/163], Loss: 0.1622\n",
      "Epoch [70/100], Step [61/163], Loss: 0.0823\n",
      "Epoch [70/100], Step [81/163], Loss: 0.0695\n",
      "Epoch [70/100], Step [101/163], Loss: 0.0288\n",
      "Epoch [70/100], Step [121/163], Loss: 0.0351\n",
      "Epoch [70/100], Step [141/163], Loss: 0.0321\n",
      "Epoch [70/100], Step [161/163], Loss: 0.0158\n",
      "Epoch [71/100], Step [1/163], Loss: 0.0121\n",
      "Epoch [71/100], Step [21/163], Loss: 0.0249\n",
      "Epoch [71/100], Step [41/163], Loss: 0.0306\n",
      "Epoch [71/100], Step [61/163], Loss: 0.2946\n",
      "Epoch [71/100], Step [81/163], Loss: 0.6167\n",
      "Epoch [71/100], Step [101/163], Loss: 0.5138\n",
      "Epoch [71/100], Step [121/163], Loss: 0.5202\n",
      "Epoch [71/100], Step [141/163], Loss: 0.4357\n",
      "Epoch [71/100], Step [161/163], Loss: 0.0654\n",
      "Epoch [72/100], Step [1/163], Loss: 0.1527\n",
      "Epoch [72/100], Step [21/163], Loss: 0.1174\n",
      "Epoch [72/100], Step [41/163], Loss: 0.1023\n",
      "Epoch [72/100], Step [61/163], Loss: 0.1565\n",
      "Epoch [72/100], Step [81/163], Loss: 0.0344\n",
      "Epoch [72/100], Step [101/163], Loss: 0.0503\n",
      "Epoch [72/100], Step [121/163], Loss: 0.0442\n",
      "Epoch [72/100], Step [141/163], Loss: 0.0355\n",
      "Epoch [72/100], Step [161/163], Loss: 0.0509\n",
      "Epoch [73/100], Step [1/163], Loss: 0.0180\n",
      "Epoch [73/100], Step [21/163], Loss: 0.0092\n",
      "Epoch [73/100], Step [41/163], Loss: 0.0197\n",
      "Epoch [73/100], Step [61/163], Loss: 0.0454\n",
      "Epoch [73/100], Step [81/163], Loss: 0.0251\n",
      "Epoch [73/100], Step [101/163], Loss: 0.0254\n",
      "Epoch [73/100], Step [121/163], Loss: 0.0413\n",
      "Epoch [73/100], Step [141/163], Loss: 0.7055\n",
      "Epoch [73/100], Step [161/163], Loss: 0.5055\n",
      "Epoch [74/100], Step [1/163], Loss: 1.9439\n",
      "Epoch [74/100], Step [21/163], Loss: 0.2661\n",
      "Epoch [74/100], Step [41/163], Loss: 0.0861\n",
      "Epoch [74/100], Step [61/163], Loss: 0.3104\n",
      "Epoch [74/100], Step [81/163], Loss: 0.2175\n",
      "Epoch [74/100], Step [101/163], Loss: 0.0726\n",
      "Epoch [74/100], Step [121/163], Loss: 0.0445\n",
      "Epoch [74/100], Step [141/163], Loss: 0.0528\n",
      "Epoch [74/100], Step [161/163], Loss: 0.0505\n",
      "Epoch [75/100], Step [1/163], Loss: 0.0659\n",
      "Epoch [75/100], Step [21/163], Loss: 0.0812\n",
      "Epoch [75/100], Step [41/163], Loss: 0.0544\n",
      "Epoch [75/100], Step [61/163], Loss: 0.0716\n",
      "Epoch [75/100], Step [81/163], Loss: 0.1019\n",
      "Epoch [75/100], Step [101/163], Loss: 0.0602\n",
      "Epoch [75/100], Step [121/163], Loss: 0.0229\n",
      "Epoch [75/100], Step [141/163], Loss: 0.0239\n",
      "Epoch [75/100], Step [161/163], Loss: 0.0635\n",
      "Epoch [76/100], Step [1/163], Loss: 0.0366\n",
      "Epoch [76/100], Step [21/163], Loss: 0.0590\n",
      "Epoch [76/100], Step [41/163], Loss: 0.0239\n",
      "Epoch [76/100], Step [61/163], Loss: 0.0114\n",
      "Epoch [76/100], Step [81/163], Loss: 0.0226\n",
      "Epoch [76/100], Step [101/163], Loss: 0.0171\n",
      "Epoch [76/100], Step [121/163], Loss: 0.0138\n",
      "Epoch [76/100], Step [141/163], Loss: 0.0149\n",
      "Epoch [76/100], Step [161/163], Loss: 0.0655\n",
      "Epoch [77/100], Step [1/163], Loss: 0.0379\n",
      "Epoch [77/100], Step [21/163], Loss: 0.0702\n",
      "Epoch [77/100], Step [41/163], Loss: 0.0452\n",
      "Epoch [77/100], Step [61/163], Loss: 0.0229\n",
      "Epoch [77/100], Step [81/163], Loss: 0.0470\n",
      "Epoch [77/100], Step [101/163], Loss: 0.0639\n",
      "Epoch [77/100], Step [121/163], Loss: 0.0352\n",
      "Epoch [77/100], Step [141/163], Loss: 0.0183\n",
      "Epoch [77/100], Step [161/163], Loss: 0.0143\n",
      "Epoch [78/100], Step [1/163], Loss: 0.0382\n",
      "Epoch [78/100], Step [21/163], Loss: 0.0255\n",
      "Epoch [78/100], Step [41/163], Loss: 0.0361\n",
      "Epoch [78/100], Step [61/163], Loss: 0.0527\n",
      "Epoch [78/100], Step [81/163], Loss: 0.1470\n",
      "Epoch [78/100], Step [101/163], Loss: 0.4319\n",
      "Epoch [78/100], Step [121/163], Loss: 0.7588\n",
      "Epoch [78/100], Step [141/163], Loss: 0.1063\n",
      "Epoch [78/100], Step [161/163], Loss: 0.0486\n",
      "Epoch [79/100], Step [1/163], Loss: 0.0277\n",
      "Epoch [79/100], Step [21/163], Loss: 0.0332\n",
      "Epoch [79/100], Step [41/163], Loss: 0.0830\n",
      "Epoch [79/100], Step [61/163], Loss: 0.0493\n",
      "Epoch [79/100], Step [81/163], Loss: 0.0660\n",
      "Epoch [79/100], Step [101/163], Loss: 0.0782\n",
      "Epoch [79/100], Step [121/163], Loss: 0.0598\n",
      "Epoch [79/100], Step [141/163], Loss: 0.0836\n",
      "Epoch [79/100], Step [161/163], Loss: 0.1325\n",
      "Epoch [80/100], Step [1/163], Loss: 0.1958\n",
      "Epoch [80/100], Step [21/163], Loss: 0.0858\n",
      "Epoch [80/100], Step [41/163], Loss: 0.9066\n",
      "Epoch [80/100], Step [61/163], Loss: 0.2958\n",
      "Epoch [80/100], Step [81/163], Loss: 1.8770\n",
      "Epoch [80/100], Step [101/163], Loss: 0.8795\n",
      "Epoch [80/100], Step [121/163], Loss: 1.2417\n",
      "Epoch [80/100], Step [141/163], Loss: 0.3012\n",
      "Epoch [80/100], Step [161/163], Loss: 0.0914\n",
      "Epoch [81/100], Step [1/163], Loss: 0.1530\n",
      "Epoch [81/100], Step [21/163], Loss: 0.2146\n",
      "Epoch [81/100], Step [41/163], Loss: 0.0580\n",
      "Epoch [81/100], Step [61/163], Loss: 0.0409\n",
      "Epoch [81/100], Step [81/163], Loss: 0.0421\n",
      "Epoch [81/100], Step [101/163], Loss: 0.0725\n",
      "Epoch [81/100], Step [121/163], Loss: 0.0376\n",
      "Epoch [81/100], Step [141/163], Loss: 0.0694\n",
      "Epoch [81/100], Step [161/163], Loss: 0.0560\n",
      "Epoch [82/100], Step [1/163], Loss: 0.0463\n",
      "Epoch [82/100], Step [21/163], Loss: 0.0241\n",
      "Epoch [82/100], Step [41/163], Loss: 0.0264\n",
      "Epoch [82/100], Step [61/163], Loss: 0.1924\n",
      "Epoch [82/100], Step [81/163], Loss: 0.0841\n",
      "Epoch [82/100], Step [101/163], Loss: 0.0314\n",
      "Epoch [82/100], Step [121/163], Loss: 0.1038\n",
      "Epoch [82/100], Step [141/163], Loss: 0.1253\n",
      "Epoch [82/100], Step [161/163], Loss: 0.1304\n",
      "Epoch [83/100], Step [1/163], Loss: 0.0491\n",
      "Epoch [83/100], Step [21/163], Loss: 0.1713\n",
      "Epoch [83/100], Step [41/163], Loss: 0.1610\n",
      "Epoch [83/100], Step [61/163], Loss: 0.1082\n",
      "Epoch [83/100], Step [81/163], Loss: 0.1439\n",
      "Epoch [83/100], Step [101/163], Loss: 0.1246\n",
      "Epoch [83/100], Step [121/163], Loss: 0.0674\n",
      "Epoch [83/100], Step [141/163], Loss: 0.0578\n",
      "Epoch [83/100], Step [161/163], Loss: 0.2496\n",
      "Epoch [84/100], Step [1/163], Loss: 0.0735\n",
      "Epoch [84/100], Step [21/163], Loss: 0.0461\n",
      "Epoch [84/100], Step [41/163], Loss: 0.0437\n",
      "Epoch [84/100], Step [61/163], Loss: 0.4092\n",
      "Epoch [84/100], Step [81/163], Loss: 0.2688\n",
      "Epoch [84/100], Step [101/163], Loss: 0.3377\n",
      "Epoch [84/100], Step [121/163], Loss: 0.4745\n",
      "Epoch [84/100], Step [141/163], Loss: 0.1035\n",
      "Epoch [84/100], Step [161/163], Loss: 0.0837\n",
      "Epoch [85/100], Step [1/163], Loss: 0.0681\n",
      "Epoch [85/100], Step [21/163], Loss: 0.0351\n",
      "Epoch [85/100], Step [41/163], Loss: 0.0274\n",
      "Epoch [85/100], Step [61/163], Loss: 0.4571\n",
      "Epoch [85/100], Step [81/163], Loss: 0.7215\n",
      "Epoch [85/100], Step [101/163], Loss: 0.1267\n",
      "Epoch [85/100], Step [121/163], Loss: 0.1227\n",
      "Epoch [85/100], Step [141/163], Loss: 0.0253\n",
      "Epoch [85/100], Step [161/163], Loss: 0.0224\n",
      "Epoch [86/100], Step [1/163], Loss: 0.0170\n",
      "Epoch [86/100], Step [21/163], Loss: 0.0211\n",
      "Epoch [86/100], Step [41/163], Loss: 0.0071\n",
      "Epoch [86/100], Step [61/163], Loss: 0.0266\n",
      "Epoch [86/100], Step [81/163], Loss: 0.0166\n",
      "Epoch [86/100], Step [101/163], Loss: 0.0167\n",
      "Epoch [86/100], Step [121/163], Loss: 0.0222\n",
      "Epoch [86/100], Step [141/163], Loss: 0.0092\n",
      "Epoch [86/100], Step [161/163], Loss: 0.0099\n",
      "Epoch [87/100], Step [1/163], Loss: 0.0410\n",
      "Epoch [87/100], Step [21/163], Loss: 0.0196\n",
      "Epoch [87/100], Step [41/163], Loss: 0.0116\n",
      "Epoch [87/100], Step [61/163], Loss: 0.0297\n",
      "Epoch [87/100], Step [81/163], Loss: 0.0881\n",
      "Epoch [87/100], Step [101/163], Loss: 0.1554\n",
      "Epoch [87/100], Step [121/163], Loss: 0.1285\n",
      "Epoch [87/100], Step [141/163], Loss: 0.0208\n",
      "Epoch [87/100], Step [161/163], Loss: 0.0640\n",
      "Epoch [88/100], Step [1/163], Loss: 0.1648\n",
      "Epoch [88/100], Step [21/163], Loss: 0.0089\n",
      "Epoch [88/100], Step [41/163], Loss: 0.0189\n",
      "Epoch [88/100], Step [61/163], Loss: 0.0532\n",
      "Epoch [88/100], Step [81/163], Loss: 0.0098\n",
      "Epoch [88/100], Step [101/163], Loss: 0.0370\n",
      "Epoch [88/100], Step [121/163], Loss: 0.6912\n",
      "Epoch [88/100], Step [141/163], Loss: 1.6009\n",
      "Epoch [88/100], Step [161/163], Loss: 0.8792\n",
      "Epoch [89/100], Step [1/163], Loss: 0.2064\n",
      "Epoch [89/100], Step [21/163], Loss: 0.1279\n",
      "Epoch [89/100], Step [41/163], Loss: 0.2514\n",
      "Epoch [89/100], Step [61/163], Loss: 0.1599\n",
      "Epoch [89/100], Step [81/163], Loss: 0.1847\n",
      "Epoch [89/100], Step [101/163], Loss: 0.1006\n",
      "Epoch [89/100], Step [121/163], Loss: 0.0825\n",
      "Epoch [89/100], Step [141/163], Loss: 0.0657\n",
      "Epoch [89/100], Step [161/163], Loss: 0.0200\n",
      "Epoch [90/100], Step [1/163], Loss: 0.0643\n",
      "Epoch [90/100], Step [21/163], Loss: 0.0184\n",
      "Epoch [90/100], Step [41/163], Loss: 0.0211\n",
      "Epoch [90/100], Step [61/163], Loss: 0.0118\n",
      "Epoch [90/100], Step [81/163], Loss: 0.0631\n",
      "Epoch [90/100], Step [101/163], Loss: 0.0256\n",
      "Epoch [90/100], Step [121/163], Loss: 0.0181\n",
      "Epoch [90/100], Step [141/163], Loss: 0.0173\n",
      "Epoch [90/100], Step [161/163], Loss: 0.0225\n",
      "Epoch [91/100], Step [1/163], Loss: 0.0366\n",
      "Epoch [91/100], Step [21/163], Loss: 0.0328\n",
      "Epoch [91/100], Step [41/163], Loss: 0.1822\n",
      "Epoch [91/100], Step [61/163], Loss: 0.4824\n",
      "Epoch [91/100], Step [81/163], Loss: 0.2213\n",
      "Epoch [91/100], Step [101/163], Loss: 0.2401\n",
      "Epoch [91/100], Step [121/163], Loss: 0.6884\n",
      "Epoch [91/100], Step [141/163], Loss: 0.3646\n",
      "Epoch [91/100], Step [161/163], Loss: 0.2108\n",
      "Epoch [92/100], Step [1/163], Loss: 0.3581\n",
      "Epoch [92/100], Step [21/163], Loss: 0.0983\n",
      "Epoch [92/100], Step [41/163], Loss: 0.0505\n",
      "Epoch [92/100], Step [61/163], Loss: 0.0333\n",
      "Epoch [92/100], Step [81/163], Loss: 0.1096\n",
      "Epoch [92/100], Step [101/163], Loss: 0.0331\n",
      "Epoch [92/100], Step [121/163], Loss: 0.0227\n",
      "Epoch [92/100], Step [141/163], Loss: 0.0550\n",
      "Epoch [92/100], Step [161/163], Loss: 0.0092\n",
      "Epoch [93/100], Step [1/163], Loss: 0.0240\n",
      "Epoch [93/100], Step [21/163], Loss: 0.0120\n",
      "Epoch [93/100], Step [41/163], Loss: 0.0130\n",
      "Epoch [93/100], Step [61/163], Loss: 0.0154\n",
      "Epoch [93/100], Step [81/163], Loss: 0.0167\n",
      "Epoch [93/100], Step [101/163], Loss: 0.0294\n",
      "Epoch [93/100], Step [121/163], Loss: 0.0267\n",
      "Epoch [93/100], Step [141/163], Loss: 0.0338\n",
      "Epoch [93/100], Step [161/163], Loss: 0.2929\n",
      "Epoch [94/100], Step [1/163], Loss: 0.1902\n",
      "Epoch [94/100], Step [21/163], Loss: 0.6167\n",
      "Epoch [94/100], Step [41/163], Loss: 0.4290\n",
      "Epoch [94/100], Step [61/163], Loss: 0.6499\n",
      "Epoch [94/100], Step [81/163], Loss: 0.1382\n",
      "Epoch [94/100], Step [101/163], Loss: 0.0723\n",
      "Epoch [94/100], Step [121/163], Loss: 0.2313\n",
      "Epoch [94/100], Step [141/163], Loss: 1.2407\n",
      "Epoch [94/100], Step [161/163], Loss: 0.8767\n",
      "Epoch [95/100], Step [1/163], Loss: 0.9573\n",
      "Epoch [95/100], Step [21/163], Loss: 1.0733\n",
      "Epoch [95/100], Step [41/163], Loss: 0.1366\n",
      "Epoch [95/100], Step [61/163], Loss: 0.2755\n",
      "Epoch [95/100], Step [81/163], Loss: 0.0222\n",
      "Epoch [95/100], Step [101/163], Loss: 0.0237\n",
      "Epoch [95/100], Step [121/163], Loss: 0.0254\n",
      "Epoch [95/100], Step [141/163], Loss: 0.0212\n",
      "Epoch [95/100], Step [161/163], Loss: 0.0129\n",
      "Epoch [96/100], Step [1/163], Loss: 0.0796\n",
      "Epoch [96/100], Step [21/163], Loss: 0.0198\n",
      "Epoch [96/100], Step [41/163], Loss: 0.0214\n",
      "Epoch [96/100], Step [61/163], Loss: 0.0231\n",
      "Epoch [96/100], Step [81/163], Loss: 0.0132\n",
      "Epoch [96/100], Step [101/163], Loss: 0.0247\n",
      "Epoch [96/100], Step [121/163], Loss: 0.0188\n",
      "Epoch [96/100], Step [141/163], Loss: 0.0275\n",
      "Epoch [96/100], Step [161/163], Loss: 0.0625\n",
      "Epoch [97/100], Step [1/163], Loss: 0.0573\n",
      "Epoch [97/100], Step [21/163], Loss: 0.0337\n",
      "Epoch [97/100], Step [41/163], Loss: 0.1200\n",
      "Epoch [97/100], Step [61/163], Loss: 0.7143\n",
      "Epoch [97/100], Step [81/163], Loss: 0.0775\n",
      "Epoch [97/100], Step [101/163], Loss: 0.1870\n",
      "Epoch [97/100], Step [121/163], Loss: 0.0312\n",
      "Epoch [97/100], Step [141/163], Loss: 0.0786\n",
      "Epoch [97/100], Step [161/163], Loss: 0.0131\n",
      "Epoch [98/100], Step [1/163], Loss: 0.0121\n",
      "Epoch [98/100], Step [21/163], Loss: 0.0147\n",
      "Epoch [98/100], Step [41/163], Loss: 0.0100\n",
      "Epoch [98/100], Step [61/163], Loss: 0.0221\n",
      "Epoch [98/100], Step [81/163], Loss: 0.0149\n",
      "Epoch [98/100], Step [101/163], Loss: 0.0060\n",
      "Epoch [98/100], Step [121/163], Loss: 0.0154\n",
      "Epoch [98/100], Step [141/163], Loss: 0.0157\n",
      "Epoch [98/100], Step [161/163], Loss: 0.5880\n",
      "Epoch [99/100], Step [1/163], Loss: 0.3221\n",
      "Epoch [99/100], Step [21/163], Loss: 0.1022\n",
      "Epoch [99/100], Step [41/163], Loss: 0.0595\n",
      "Epoch [99/100], Step [61/163], Loss: 0.0466\n",
      "Epoch [99/100], Step [81/163], Loss: 0.0247\n",
      "Epoch [99/100], Step [101/163], Loss: 0.0276\n",
      "Epoch [99/100], Step [121/163], Loss: 0.0253\n",
      "Epoch [99/100], Step [141/163], Loss: 0.0336\n",
      "Epoch [99/100], Step [161/163], Loss: 0.0235\n",
      "Epoch [100/100], Step [1/163], Loss: 0.0257\n",
      "Epoch [100/100], Step [21/163], Loss: 0.0248\n",
      "Epoch [100/100], Step [41/163], Loss: 0.0954\n",
      "Epoch [100/100], Step [61/163], Loss: 0.2224\n",
      "Epoch [100/100], Step [81/163], Loss: 0.3930\n",
      "Epoch [100/100], Step [101/163], Loss: 0.5601\n",
      "Epoch [100/100], Step [121/163], Loss: 1.0597\n",
      "Epoch [100/100], Step [141/163], Loss: 0.4415\n",
      "Epoch [100/100], Step [161/163], Loss: 0.1895\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49702d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next 6 numbers for contest 2602 of Mega Sena are: [7, 24, 14, 6, 50, 5]\n"
     ]
    }
   ],
   "source": [
    "# Define the contest number and extract the last 6 winning numbers\n",
    "contest_number = data['Conc'].max() + 1 # assuming the next contest number is one greater than the max contest number in the dataset\n",
    "last_six_numbers = data.loc[data['Conc'] == contest_number - 1, ['NR1', 'NR2', 'NR3', 'NR4', 'NR5', 'NR6']].values.ravel()\n",
    "\n",
    "# Generate next 6 numbers\n",
    "next_six_numbers = []\n",
    "while len(next_six_numbers) < 6:\n",
    "    num = np.random.randint(1, 61) # assuming Mega Sena has 60 balls\n",
    "    if num not in last_six_numbers and num not in next_six_numbers:\n",
    "        next_six_numbers.append(num)\n",
    "\n",
    "print(f\"The next 6 numbers for contest {contest_number} of Mega Sena are: {next_six_numbers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ff0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
