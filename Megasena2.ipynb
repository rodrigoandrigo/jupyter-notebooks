{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af652f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cea6253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 9585761092214872990\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 22561734528\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 315813598804755119\n",
       " physical_device_desc: \"device: 0, name: DML, pci bus id: <undefined>\"\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2789e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conc</th>\n",
       "      <th>Data</th>\n",
       "      <th>NR1</th>\n",
       "      <th>NR2</th>\n",
       "      <th>NR3</th>\n",
       "      <th>NR4</th>\n",
       "      <th>NR5</th>\n",
       "      <th>NR6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2601</td>\n",
       "      <td>14/06/2023</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2600</td>\n",
       "      <td>10/06/2023</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>46</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2599</td>\n",
       "      <td>07/06/2023</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2598</td>\n",
       "      <td>03/06/2023</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>53</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2597</td>\n",
       "      <td>31/05/2023</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>56</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2596</td>\n",
       "      <td>27/05/2023</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>47</td>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2595</td>\n",
       "      <td>24/05/2023</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2594</td>\n",
       "      <td>20/05/2023</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>49</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2593</td>\n",
       "      <td>17/05/2023</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2592</td>\n",
       "      <td>13/05/2023</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Conc        Data  NR1  NR2  NR3  NR4  NR5  NR6\n",
       "0  2601  14/06/2023    3    8   34   40   44   55\n",
       "1  2600  10/06/2023    4   18   37   38   46   60\n",
       "2  2599  07/06/2023   23   28   34   43   47   60\n",
       "3  2598  03/06/2023    7   14   24   53   58   60\n",
       "4  2597  31/05/2023   14   26   34   54   56   58\n",
       "5  2596  27/05/2023   34   35   39   47   51   56\n",
       "6  2595  24/05/2023    1   13   34   39   50   52\n",
       "7  2594  20/05/2023    7   26   32   35   49   55\n",
       "8  2593  17/05/2023   10   14   17   25   32   39\n",
       "9  2592  13/05/2023   15   17   28   34   35   51"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"Resultados-MegaSena.csv\", encoding='latin-1')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "943d5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the features and labels\n",
    "data['Data'] = pd.to_datetime(data['Data'], format='%d/%m/%Y')\n",
    "features = data[['Data', 'Conc']]\n",
    "labels = data[['NR1', 'NR2', 'NR3', 'NR4', 'NR5', 'NR6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column to numerical features\n",
    "features['Data'] = pd.to_datetime(features['Data'])\n",
    "features['Day'] = features['Data'].dt.day \n",
    "features['Month'] = features['Data'].dt.month\n",
    "features['Year'] = features['Data'].dt.year\n",
    "#features['DataI'] = pd.to_numeric(features['Data'])\n",
    "#features['DataF'] = features['Data'].apply(lambda x: x.timestamp())\n",
    "features.drop(['Data'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4858eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Conc  Day  Month  Year\n",
      "0     2601   14      6  2023\n",
      "1     2600   10      6  2023\n",
      "2     2599    7      6  2023\n",
      "3     2598    3      6  2023\n",
      "4     2597   31      5  2023\n",
      "...    ...  ...    ...   ...\n",
      "2596     5    8      4  1996\n",
      "2597     4    1      4  1996\n",
      "2598     3   25      3  1996\n",
      "2599     2   18      3  1996\n",
      "2600     1   11      3  1996\n",
      "\n",
      "[2601 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242d50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features (year should have more weightage than day and month)\n",
    "#features['Day'] = features['Day'] / 31.0\n",
    "#features['Month'] = features['Month'] / 12.0\n",
    "#features['Year'] = (features['Year'] - 2000) / 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3db0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features.values, labels.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e66bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, input_shape=[4], activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e729708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "687ae140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 5ms/step - loss: 1228.7699 - val_loss: 1217.2827\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4656 - val_loss: 1217.2827\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4648 - val_loss: 1217.2827\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4651 - val_loss: 1217.2827\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4650 - val_loss: 1217.2827\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4653 - val_loss: 1217.2827\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 1228.4652 - val_loss: 1217.2827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213ca13f408>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(train_features, train_labels, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e0eaa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on test data\n",
    "test_predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b210661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next 6 numbers for contest 2602 of Mega Sena are: [9, 39, 49, 42, 50, 57]\n"
     ]
    }
   ],
   "source": [
    "# Define the contest number and extract the last 6 winning numbers\n",
    "contest_number = data['Conc'].max() + 1 # assuming the next contest number is one greater than the max contest number in the dataset\n",
    "last_six_numbers = data.loc[data['Conc'] == contest_number - 1, ['NR1', 'NR2', 'NR3', 'NR4', 'NR5', 'NR6']].values.ravel()\n",
    "\n",
    "# Generate next 6 numbers\n",
    "next_six_numbers = []\n",
    "while len(next_six_numbers) < 6:\n",
    "    num = np.random.randint(1, 61) # assuming Mega Sena has 60 balls\n",
    "    if num not in last_six_numbers and num not in next_six_numbers:\n",
    "        next_six_numbers.append(num)\n",
    "\n",
    "print(f\"The next 6 numbers for contest {contest_number} of Mega Sena are: {next_six_numbers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c29fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
