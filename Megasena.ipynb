{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af652f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cea6253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6734411662548319663\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 22561734528\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 15158705708090860516\n",
       " physical_device_desc: \"device: 0, name: DML, pci bus id: <undefined>\"\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2789e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conc</th>\n",
       "      <th>Data</th>\n",
       "      <th>NR1</th>\n",
       "      <th>NR2</th>\n",
       "      <th>NR3</th>\n",
       "      <th>NR4</th>\n",
       "      <th>NR5</th>\n",
       "      <th>NR6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2601</td>\n",
       "      <td>14/06/2023</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2600</td>\n",
       "      <td>10/06/2023</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>46</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2599</td>\n",
       "      <td>07/06/2023</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2598</td>\n",
       "      <td>03/06/2023</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>53</td>\n",
       "      <td>58</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2597</td>\n",
       "      <td>31/05/2023</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>56</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Conc        Data  NR1  NR2  NR3  NR4  NR5  NR6\n",
       "0  2601  14/06/2023    3    8   34   40   44   55\n",
       "1  2600  10/06/2023    4   18   37   38   46   60\n",
       "2  2599  07/06/2023   23   28   34   43   47   60\n",
       "3  2598  03/06/2023    7   14   24   53   58   60\n",
       "4  2597  31/05/2023   14   26   34   54   56   58"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"Resultados-MegaSena.csv\", encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "943d5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and target variable\n",
    "X = df[['Conc','NR1', 'NR2', 'NR3', 'NR4','NR5','NR6']].values\n",
    "y = df[['Conc','NR1', 'NR2', 'NR3', 'NR4','NR5','NR6']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6685b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3db0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e66bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(16, activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dense(512, activation='relu'),\n",
    "  tf.keras.layers.Dense(7, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e729708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "687ae140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 7ms/step - loss: 232036.6406 - val_loss: 56299.2695\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 32410.1191 - val_loss: 15594.5596\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 11024.1348 - val_loss: 7032.9546\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 6422.1191 - val_loss: 4274.3589\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 3838.9292 - val_loss: 2954.8213\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 2540.1648 - val_loss: 2065.0029\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1825.5375 - val_loss: 1598.9335\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1458.2368 - val_loss: 1339.1510\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 1201.0933 - val_loss: 1170.5936\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 1024.6254 - val_loss: 986.3531\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 932.9973 - val_loss: 915.4726\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 824.4918 - val_loss: 843.0073\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 766.6808 - val_loss: 797.9907\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 709.4861 - val_loss: 744.5822\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 664.8057 - val_loss: 702.9405\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 631.5698 - val_loss: 652.1025\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 607.2934 - val_loss: 658.9990\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 609.7230 - val_loss: 618.5507\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 561.3123 - val_loss: 580.8535\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 542.3932 - val_loss: 547.0975\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 505.9628 - val_loss: 526.4169\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 482.7010 - val_loss: 499.0366\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 440.9276 - val_loss: 445.1607\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 368.7645 - val_loss: 343.5544\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 258.4868 - val_loss: 249.9437\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 225.3433 - val_loss: 309.5411\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 211.5107 - val_loss: 229.0280\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 183.6005 - val_loss: 212.2144\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 180.4475 - val_loss: 192.2794\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 168.3196 - val_loss: 198.1135\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 158.6789 - val_loss: 194.0689\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 149.4664 - val_loss: 165.0242\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 139.4533 - val_loss: 193.7115\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 137.0216 - val_loss: 158.1677\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 127.5108 - val_loss: 152.2426\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 125.8296 - val_loss: 140.1362\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 125.1569 - val_loss: 153.8352\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 113.3209 - val_loss: 128.1541\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 107.1052 - val_loss: 126.9103\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 106.6593 - val_loss: 113.9498\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 110.6911 - val_loss: 122.0395\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 97.2597 - val_loss: 104.4400\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 106.0096 - val_loss: 113.7343\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 98.4407 - val_loss: 103.0673\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 87.2947 - val_loss: 97.0910\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 82.1641 - val_loss: 111.2200\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 103.5944 - val_loss: 95.6451\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 101.1222 - val_loss: 98.9344\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 74.8553 - val_loss: 93.7233\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 80.1219 - val_loss: 83.5414\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 76.1135 - val_loss: 97.2653\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 81.1305 - val_loss: 88.4127\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 73.0298 - val_loss: 76.2816\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 69.3624 - val_loss: 81.1344\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 76.9312 - val_loss: 80.9632\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 73.0460 - val_loss: 70.3155\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 59.1647 - val_loss: 76.1981\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 63.9216 - val_loss: 74.9626\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 63.8809 - val_loss: 75.5821\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 60.2771 - val_loss: 78.3822\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 63.3292 - val_loss: 69.5837\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 57.5379 - val_loss: 68.6308\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 53.5854 - val_loss: 81.8457\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 52.9310 - val_loss: 60.5356\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 52.7813 - val_loss: 63.5390\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 57.5700 - val_loss: 59.2496\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 49.0716 - val_loss: 61.9907\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 51.4897 - val_loss: 57.1023\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 46.7343 - val_loss: 60.9600\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 52.6154 - val_loss: 55.6826\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 55.7877 - val_loss: 64.5710\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 47.5504 - val_loss: 90.1550\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 103.2840 - val_loss: 53.7551\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 52.0912 - val_loss: 52.4039\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 49.2339 - val_loss: 54.2101\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 40.6892 - val_loss: 46.3790\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 49.9443 - val_loss: 53.7494\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 38.9400 - val_loss: 47.5163\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 40.1457 - val_loss: 47.2120\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 56.0034 - val_loss: 127.7310\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 58.8432 - val_loss: 50.7025\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 37.2790 - val_loss: 51.6934\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 45.1553 - val_loss: 86.6586\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 47.8613 - val_loss: 43.9612\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 38.4112 - val_loss: 45.8008\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 48.4724 - val_loss: 62.9962\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 40.8194 - val_loss: 56.0850\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 38.3660 - val_loss: 64.5976\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 45.7474 - val_loss: 46.8075\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 36.9716 - val_loss: 41.9973\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 38.8446 - val_loss: 38.9632\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 32.2367 - val_loss: 40.9254\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 83.7056 - val_loss: 59.7914\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 41.7050 - val_loss: 43.9320\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 33.6101 - val_loss: 62.5866\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 39.4790 - val_loss: 51.7931\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 90.5185 - val_loss: 63.8236\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 56.4577 - val_loss: 50.5136\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 31.6256 - val_loss: 37.1877\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 29.0745 - val_loss: 49.7389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23036821208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e0eaa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 4ms/step - loss: 59.8531\n",
      "Test loss: 59.85309600830078\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b210661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next 6 numbers for contest 2602 of Mega Sena are: [26, 1, 23, 6, 43, 17]\n"
     ]
    }
   ],
   "source": [
    "# Define the contest number and extract the last 6 winning numbers\n",
    "contest_number = df['Conc'].max() + 1 # assuming the next contest number is one greater than the max contest number in the dataset\n",
    "last_six_numbers = df.loc[df['Conc'] == contest_number - 1, ['NR1', 'NR2', 'NR3', 'NR4', 'NR5', 'NR6']].values.ravel()\n",
    "\n",
    "# Generate next 6 numbers\n",
    "next_six_numbers = []\n",
    "while len(next_six_numbers) < 6:\n",
    "    num = np.random.randint(1, 61) # assuming Mega Sena has 60 balls\n",
    "    if num not in last_six_numbers and num not in next_six_numbers:\n",
    "        next_six_numbers.append(num)\n",
    "\n",
    "print(f\"The next 6 numbers for contest {contest_number} of Mega Sena are: {next_six_numbers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109e03d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
